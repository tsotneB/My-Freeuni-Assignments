{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a704ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torch in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: torchtext in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: torch==1.11.0 in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from torchtext) (1.11.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from torchtext) (2.26.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from torchtext) (1.20.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from torchtext) (4.62.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from torch==1.11.0->torchtext) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from requests->torchtext) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n",
      "Requirement already satisfied: mosestokenizer in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: docopt in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from mosestokenizer) (0.6.2)\n",
      "Requirement already satisfied: uctools in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from mosestokenizer) (1.3.0)\n",
      "Requirement already satisfied: openfile in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from mosestokenizer) (0.0.7)\n",
      "Requirement already satisfied: toolwrapper in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from mosestokenizer) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install torch\n",
    "!pip install torchtext\n",
    "!pip install mosestokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa562b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\tsotn\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f1238",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cb6f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from mosestokenizer import *\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab96321",
   "metadata": {},
   "source": [
    "# Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bea13839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load('../resources/georgian_word2vec.model')\n",
    "w2v = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7666472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pd.read_csv('../data/data.csv',nrows=100000).values.flatten()\n",
    "nrows = len(sequences)\n",
    "train = sequences[:int(nrows*0.9)]\n",
    "test = sequences[int(nrows*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683acf3b",
   "metadata": {},
   "source": [
    "# Words To Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35737792",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = collections.Counter()\n",
    "word_counts['<pad>'] = 1\n",
    "\n",
    "for sentence in train:\n",
    "    for word in sentence.split():\n",
    "        word_counts[word] += 1\n",
    "        \n",
    "unique_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "vocab_size = len(unique_words) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35e958e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index: word for index, word in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9ff59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(unique_words)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ca927",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e249609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_embedings = {}\n",
    "\n",
    "for idx, token in index_to_word.items():\n",
    "    try:\n",
    "        index_to_embedings[idx] = w2v[token]\n",
    "    except:\n",
    "        index_to_embedings[idx] = torch.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43fcd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = [x for _, x in index_to_embedings.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be15f2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06287895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,*, n_vocab, num_layers, embedding_dim, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.emb = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_vectors), \n",
    "            freeze=False\n",
    "        ) \n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear( self.hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        x = self.emb(x)\n",
    "        output, state = self.lstm(x, (h0, c0))\n",
    "        logits = self.fc(output)\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40267c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(\n",
    "    n_vocab=139769, # vocab_size\n",
    "    num_layers=2,\n",
    "    embedding_dim = 100,\n",
    "    hidden_size= 128\n",
    ")\n",
    "model.load_state_dict(torch.load('../resources/nn_lstm_model_state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e51161d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(w):\n",
    "    try:\n",
    "        return word_to_index[w]\n",
    "    except:\n",
    "        return word_to_index['<pad>']\n",
    "def predict(model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[word2index(w) for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x)\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        while(index_to_word[word_index] == '<pad>'):\n",
    "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(index_to_word[word_index])\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ee45adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ბიჭი ჩვენს განცხადებებს გავიდა # #'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, 'ბიჭი', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c291369",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sequence_len = (sum([len(x.split()) for x in sequences]) // len(sequences))\n",
    "pad_index = word_to_index['<pad>']\n",
    "n_gram_size = avg_sequence_len // 2\n",
    "test_input = []\n",
    "for i in range(len(test)):\n",
    "    test[i] = [pad_index if w not in word_to_index else word_to_index[w] for w in train[i].split()]\n",
    "def pad(x):\n",
    "    if len(x) < avg_sequence_len:\n",
    "            x += [pad_index] * (avg_sequence_len - len(x))\n",
    "    return x\n",
    "\n",
    "for line in test:\n",
    "    pad(line)\n",
    "    for i in range(len(line) - n_gram_size):\n",
    "        n_gram_sequence = line[i:i+n_gram_size]\n",
    "        test_input.append(n_gram_sequence)\n",
    "# pad or strip data, also add <eos>\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for x in batch:\n",
    "        label_list.append(x[1:])\n",
    "        x = x[:-1]\n",
    "        text_list.append(torch.Tensor(x).long())\n",
    "    text_list = torch.stack(text_list, dim=0)\n",
    "    label_list = torch.Tensor(label_list).long()\n",
    "    \n",
    "    return text_list, label_list\n",
    "test_dl = DataLoader(test_input, \n",
    "                      batch_size=128, \n",
    "                      shuffle=False, \n",
    "                      collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9eef24",
   "metadata": {},
   "source": [
    "# Computing perplexity on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538f833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98b75652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "643it [06:49,  1.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(812.8319)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_perplexity(model, dl):\n",
    "    \"\"\"\n",
    "    Compute perplexity\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad(): # tells Pytorch not to store values of intermediate computations for backward pass because we not gonna need gradients.\n",
    "        loss = 0\n",
    "        for batch, (x, y) in tqdm(enumerate(dl)):\n",
    "            y_pred, (state_h, state_c) = model(x)\n",
    "            loss += criterion(y_pred.transpose(1, 2), y)\n",
    "    model.train()\n",
    "    return np.exp(loss / len(dl))\n",
    "    \n",
    "compute_perplexity(model, test_dl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
